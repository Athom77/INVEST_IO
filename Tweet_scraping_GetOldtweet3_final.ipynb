{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet_scraping_GetOldtweet3_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYHNQEhOyx7X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "2e61ff24-0279-445e-aa57-d1d50e6c8369"
      },
      "source": [
        "#import libraries\n",
        "\n",
        "!pip install GetOldTweets3\n",
        "import GetOldTweets3 as got\n",
        "import importlib\n",
        "from textblob import TextBlob\n",
        "import datetime as dt\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "#import google translate API to pass the tweet text and translate it in english.\n",
        "!pip install googletrans\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "\n",
        "\n",
        "# re = python module for regular expression\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "consumer_key = 'Vzk6O5t967f1n4iz9iQcsQwhf'\n",
        "consumer_secret = 'Q8sncdY1j85mwNitOqb5Spcg6U6uuZ0E2KNHHZEYF2J4DdCyVi'\n",
        "access_token = '164224789-mFmU5YhessCNvubNojgfL605tbU0vh0NOna3HdQU'\n",
        "access_secret = 'Ic5LNOH2QqxziSq4eTQdJvbXo3noTcQDUzCKXgFcdPzpC'\n",
        "\n",
        "\n",
        "# Importing GetOldTweets3\n",
        "\n",
        "def get_tweets(username, top_only, start_date, end_date, max_tweets):\n",
        "   \n",
        "    # specifying tweet search criteria \n",
        "    tweetCriteria = got.manager.TweetCriteria().setUsername(username)\\\n",
        "                          .setTopTweets(top_only)\\\n",
        "                          .setSince(start_date)\\\n",
        "                          .setUntil(end_date)\\\n",
        "                          .setMaxTweets(max_tweets)\n",
        "    \n",
        "    # scraping tweets based on criteria\n",
        "    tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
        "    \n",
        "    # creating list of tweets with the tweet attributes \n",
        "    # specified in the list comprehension\n",
        "    text_tweets = [[tw.date,\n",
        "                tw.id,\n",
        "                tw.username,\n",
        "                tw.permalink,\n",
        "                tw.text,\n",
        "                tw.hashtags,\n",
        "                tw.favorites,\n",
        "                tw.geo,\n",
        "                tw.retweets,\n",
        "                tw.mentions] for tw in tweet]\n",
        "    \n",
        "    # creating dataframe, assigning column names to list of\n",
        "    # tweets corresponding to tweet attributes\n",
        "    news_df = pd.DataFrame(text_tweets, \n",
        "                            columns = ['created_at', 'id', 'user', 'link', 'text', 'hashtags', 'favorite_count', 'location', 'retweet', 'user_mentions'])\n",
        "    \n",
        "    return news_df\n",
        "\n",
        "#2 SENTIMENT ANALYSIS\n",
        "# # # # TWITTER STREAM LISTENER - Analyzer # # # #\n",
        "# we can use TextBlob for tweet sentiment analysis and determine the polarity of tweet, from -3 to 3;\n",
        "# -3 = negative, 0 = neutral, 3 = positive\n",
        " \n",
        "class TweetAnalyzer():\n",
        "    \"\"\"\n",
        "    Functionality for analyzing and categorizing content from tweets.\n",
        "    \"\"\"\n",
        "    # claen_tweet using the re module to clean tweets text \n",
        "    def clean_tweet(self, tweet):\n",
        "        return str(' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()))\n",
        "    def translator(self, tweet):\n",
        "        translator = Translator()\n",
        "        tweet = str(translator.translate(tweet))\n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: GetOldTweets3 in /usr/local/lib/python3.6/dist-packages (0.0.11)\n",
            "Requirement already satisfied: pyquery>=1.2.10 in /usr/local/lib/python3.6/dist-packages (from GetOldTweets3) (1.4.1)\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from GetOldTweets3) (4.2.6)\n",
            "Requirement already satisfied: cssselect>0.7.9 in /usr/local/lib/python3.6/dist-packages (from pyquery>=1.2.10->GetOldTweets3) (1.1.0)\n",
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.6/dist-packages (from googletrans) (0.13.3)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (1.4.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (0.9.1)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2.10)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2020.9.15)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2020.6.20)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (1.1.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.6/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.6/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (0.9.0)\n",
            "Requirement already satisfied: contextvars>=2.1; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from sniffio->httpx==0.13.3->googletrans) (2.4)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.6/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars>=2.1; python_version < \"3.7\"->sniffio->httpx==0.13.3->googletrans) (0.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ePL3kWzIe_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "58e4476d-4c44-4b14-fb00-918a55aa61da"
      },
      "source": [
        "# Defining news sources I want to include\n",
        "news_sources = \"$AAPL\"\n",
        "\n",
        "# news_sources = ['$AAPL', '$AMZN', '$MSF', '$GOOGL', '$FB', '$BABA', '$0700', '$BRK', '$V', '$WMT', '$JNJ', '$TSM', '$TSLA', '$PG']\n",
        "# getting tweets from the defined new sources, \n",
        "# only including top tweets, \n",
        "# looking at the past week with the end_date not inclusive,\n",
        "# and specifying that we want a max number of tweets = 100.\n",
        "# also sorting the tweets by date, descending.\n",
        "\n",
        "\n",
        "initial_date = \"2020-01-01\"\n",
        "final_date = \"2020-09-15\"\n",
        "max_tweet = 10000  #per keyword\n",
        "\n",
        "news_df = get_tweets(news_sources, top_only = True, start_date = initial_date, end_date = final_date, max_tweets = max_tweet).sort_values('created_at', ascending=False)\n",
        "\n",
        "\n",
        "# getting the tweet full text \n",
        "pd.set_option(\"display.max_colwidth\", -1)\n",
        "\n",
        "# adding the new items 'date' and 'ticker'\n",
        "ticker = str(news_sources)\n",
        "\n",
        "news_df[\"date\"] = news_df[\"created_at\"].dt.strftime(\"%Y-%m-%d\")\n",
        "news_df[\"ticker\"] = f'{news_sources.replace(\"$\", \"\")}'\n",
        "\n",
        "# call the class TweetAnalyzer before fill in the column 'sentiment'\n",
        "tweet_analyzer = TweetAnalyzer()\n",
        "\n",
        "# ETL on tweets: cleaning, translation, sentiment analysis:\n",
        "\n",
        "news_df['text1'] = np.array([tweet_analyzer.clean_tweet(tweet) for tweet in news_df['text']])\n",
        "news_df['text1'] = np.array([tweet_analyzer.translator(tweet) for tweet in news_df['text']])\n",
        "\n",
        "#text language detection\n",
        "\n",
        "translator = Translator()\n",
        "text1 = str(news_df['text'])\n",
        "\n",
        "declan = translator.detect(text1)\n",
        "dt1 = str(declan)\n",
        "text_lan = str.upper(dt1[14:17][0:2])\n",
        "\n",
        "news_df['text_language'] = np.array(text_lan)\n",
        "\n",
        "#sentiment analysis\n",
        "\n",
        "polarity = lambda x: TextBlob(x).sentiment.polarity\n",
        "subjectivity = lambda x: TextBlob(x).sentiment.subjectivity\n",
        "\n",
        "news_df['polarity'] = news_df['text'].astype(str).apply(polarity)\n",
        "news_df['subjectivity'] = news_df['text'].astype(str).apply(subjectivity)\n",
        "\n",
        "# Creating function for calculating positive, negative and neutral\n",
        "# More than 1 --> Positive, equal to 0 --> neutral and less than 0 --> Negative\n",
        "def ratio(x):\n",
        " if x > 0:\n",
        "  return 3\n",
        " elif x == 0:\n",
        "   return 0\n",
        " else:\n",
        "  return -3\n",
        "\n",
        "news_df['sentiment_analysis'] = news_df['polarity'].apply(ratio)\n",
        "\n",
        "#salvataggio file csv con stringa del TS convertito in data\n",
        "\n",
        "news_df = news_df[['date', 'ticker', 'user', 'link', 'created_at', 'id', 'text', 'hashtags', 'text_language', 'text1','favorite_count', 'location', 'retweet', 'user_mentions', 'polarity', 'subjectivity', 'sentiment_analysis']]\n",
        "\n",
        "news_df.to_csv(f'tweet_{initial_date.replace(\"-\", \"\")}' + \"_\" + f'{final_date.replace(\"-\", \"\")}' + \"_\" +  f'{news_sources.replace(\"$\" , \"\")}' + \".csv\", index=False)\n",
        "\n",
        "news_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5756cc1a50c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmax_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m  \u001b[0;31m#per keyword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mnews_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_sources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_tweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_tweets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41MhHoaqVWVd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e3d6b26b-e677-481d-96f4-ba79810f52fd"
      },
      "source": [
        "#statistics\n",
        "news_df['sentiment_analysis'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    5\n",
              "0    5\n",
              "Name: sentiment_analysis, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccyTMoLpVvNY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "fcb46fa9-5996-4c88-a4c7-7128566a614d"
      },
      "source": [
        "# Plotting\n",
        "\n",
        "news_df['sentiment_analysis'].value_counts().plot(kind = 'bar')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}